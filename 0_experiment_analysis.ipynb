{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with Accordion, AllSmall, and ClassFL\n",
    "\n",
    "In this notebook, we are going to discuss the results of the experiments with 3 different setup for federated learning with heterogenous devices.\n",
    "\n",
    "In our setting, we have $N = 20$ devices with their own dataset for a common task. Due to privacy reasons, they cannot share their data, hence we consider a federated learning setup where they do local training and share their updates with a central server which holds the global model, as in the classical FedAvg setup.\n",
    "\n",
    "The difference between the classical FedAvg and our setup is that each device have their own computation and communication capabilities, which limits them to training and running a model of certain size. In particular, we would like to train a model $\\mathcal{M}$ and device $j$ can run a fraction $r_j$ of the model $\\mathcal{M}$. Hence, we have a list of model fractions $R = [r_j | j \\in [N]]$ which also induces a probability distribution $\\mathbb{P}(r) = \\frac{|\\{ j \\in [N] |  r_j = r\\}|}{N}$ over model fractions at training and inference times.\n",
    "\n",
    "There are multiple ways to approach this kind of client heterogeneity:\n",
    "\n",
    "- One can train a model of size $r = \\inf R$ so that all of the devices can run and train the model. However, this might cause very low accuracies at the end of training for difficult tasks. We call this approach `AllSmall` and include it in our experiments for comparision.\n",
    "\n",
    "- One can divide all devices into classes in each of which all devices can run the same model fraction and train a separate model for each class with their own model fraction. However, this will cause each class to be limited to their own data, which can again cause in low accuracies or slow training.\n",
    "\n",
    "- Finally, we propose the Accordion approach, which we described in detail in our paper. \n",
    "\n",
    "The experiments were done with CIFAR-100 dataset and using the ResNet-56 model which has 27 adaptable residual blocks. The number of blocks each client can use is given in the cell below. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3b333b579749e1afd7d41b56c85350d5e8c7430dec91be45626406ef829933a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
